# -*- coding: utf-8 -*-
"""bi-lstm_with_data_manager.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B8hu3aUeT-OtsNBAdqTIx9H0CLJbbsyZ

# SentiMix Spanglish

# Import
"""

import numpy as np
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report, confusion_matrix
from gensim.models import KeyedVectors

import nltk

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.models import Model
from keras.optimizers import Adam
from collections import defaultdict

from keras.layers import *

from data.data_manager import Data, Preprocessor, Explorer
from baseline import print_cm


############################################

def make_lang_dict(words, tags):
    w2l = defaultdict()
    for word_list, tag_list in zip(words, tags):
        for word, tag in zip(word_list, tag_list):
            w2l[word] = tag
    return w2l


def embed_coverage(w2l, embed_en, embed_es):
    found = []
    not_found = []

    for w, l in w2l.items():
        w = w.lower()
        if w.startswith('@'):
            w = '@user'
        if (w in embed_en) or (w in embed_es):
            found.append((w, l))
        elif (w not in embed_en) and (w not in embed_es):
            not_found.append((w, l))
        else:
            print('{} does something really weird'.format((w, l)))
    return found, not_found


def preprocess_docs(train, test, unk, pad):
    print('Preprocessing documents..')
    tok = Tokenizer(oov_token=unk)
    tok.fit_on_texts(train)

    train_seq = tok.texts_to_sequences(train)
    test_seq = tok.texts_to_sequences(test)

    tok.index_word[0] = pad
    tok.word_index[pad] = 0

    max_len = max([len(t) for t in train_seq])
    print('max_len: {}'.format(max_len))

    train_pad = pad_sequences(train_seq, maxlen=max_len)
    test_pad = pad_sequences(test_seq, maxlen=max_len)

    return train_pad, test_pad, tok, max_len


def preprocess_labels(train_labels, test_labels):
    print('Preprocessing labels..')
    lb = LabelBinarizer()
    lb.fit(train_labels)

    train_cat = lb.transform(train_labels)
    test_cat = lb.transform(test_labels)

    label2index = {label: idx for idx, label in enumerate(lb.classes_)}
    index2label = {idx: label for idx, label in enumerate(lb.classes_)}

    return train_cat, test_cat, label2index, index2label


""" Embedding Preparation"""


def get_embeddings(tokenizer, embed_en, embed_es):
    i2w = tokenizer.index_word

    # make an index2embedding dict
    # use word embeddings from Spanish and English

    index2emb = dict()

    emb_list = [embed_en, embed_es]

    for i, w in i2w.items():
        index2emb[i] = embed_en['unk']
        for emb in emb_list:
            try:
                index2emb[i] = emb[w]
            except KeyError:
                continue

    return index2emb


def get_embedding_layer(index2emb, max_len, tokenizer):
    embed_size = len(index2emb[5])
    w2i = tokenizer.word_index

    # create embedding matrix
    embedding_matrix = np.zeros((len(w2i) + 1, embed_size))
    for word, i in w2i.items():
        embedding_vector = index2emb[i]
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector
        else:
            print('zero-word:', word)

    # load embedding matrix into embedding layer

    embedding_layer = Embedding(len(w2i) + 1,
                                embed_size,
                                weights=[embedding_matrix],
                                input_length=max_len,
                                trainable=False)
    return embedding_layer


def make_LSTM(embed_layer, max_len):
    print('Building Bi-LSTM with pre-trained embeddings...')
    sequence_input = Input(shape=(max_len,), dtype='int32')
    embedded_sequences = embed_layer(sequence_input)

    output_1 = Bidirectional(
        LSTM(50, activation='relu'))(embedded_sequences)
    drop = Dropout(0.3)(output_1)
    dense1 = Dense(100, activation='relu')(drop)
    drop2 = Dropout(0.3)(dense1)
    dense2 = Dense(100, activation='relu')(drop2)
    preds = Dense(3, activation='softmax')(dense2)

    model = Model(inputs=sequence_input, outputs=preds)
    adam = Adam(lr=0.001, amsgrad=True)
    model.compile(loss='categorical_crossentropy',
                  optimizer=adam, metrics=['accuracy'])

    return model


def make_LSTM_with_embeddings(vocab, max_len):
    print('Building Bi-LSTM and training embeddings...')
    sequence_input = Input(shape=(max_len,), dtype='int32')
    embedded_sequences = Embedding(input_dim=vocab,
                                   output_dim=100,
                                   input_length=max_len)(sequence_input)

    output_1 = Bidirectional(
        LSTM(50, activation='relu'))(embedded_sequences)
    drop = Dropout(0.3)(output_1)
    dense1 = Dense(20, activation='relu')(drop)
    drop2 = Dropout(0.2)(dense1)
    preds = Dense(3, activation='softmax')(drop2)

    model_wE = Model(inputs=sequence_input, outputs=preds)
    adam = Adam(lr=0.001, amsgrad=True)
    model_wE.compile(loss='categorical_crossentropy',
                     optimizer=adam, metrics=['accuracy'])

    return model_wE


def run_model(train, test, en_emb, es_emb):
    data_path = "./data/"

    if en_emb is not None:
        print('Load English embeddings..')
        embed_en = KeyedVectors.load_word2vec_format(en_emb, binary=False)
        print('Load Spanish embeddings..')
        embed_es = KeyedVectors.load_word2vec_format(es_emb, binary=False)

    # emoji informativity
    info_path = data_path + 'emoji_informativity.txt'

    # data = data_manager.Preprocessor.emoji_to_word(data, info_path)
    # data_trial = data_manager.Preprocessor.emoji_to_word(data_trial, info_path)

    # train.scramble()

    # Spanglish 2016
    # data_2016 = Data(spanglish_new, format='json')
    # data_2016 = Preprocessor.remove_emoji(data_2016)
    # data_2016 = Preprocessor.balance_data(data_2016)
    #
    # # data = Preprocessor.emoji_to_word(data, info_path)
    #
    # train = Preprocessor.combine_data(data_train, data_2016)
    # train.scramble()
    # test = data_trial

    print(f"Number of training documents: {len(train.documents)}")
    print(f"Number of testing documents: {len(test.documents)}")

    """### Check Embedding Coverage"""

    # Print the embedding coverage
    if en_emb is not None:
        data = Preprocessor.combine_data(train, test)
        word2lang_all = make_lang_dict(data.documents, data.labels)
        found, not_found = embed_coverage(word2lang_all, embed_en, embed_es)
        all_words = len(word2lang_all)
        print("{0} of {1} words were found in 2 embeddings. That is {2:2f} percent".format(len(found), all_words,
                                                                                           (len(found) / all_words * 100)))
        print("{0} of {1} words were not found. That is {2:2f} percent".format(len(not_found), all_words,
                                                                               (len(not_found) / all_words * 100)))

    ################# Pre-processing #################

    oov = 'unk'
    pad = 'PAD'

    Xtrain, Xtest, tokenizer, max_len = preprocess_docs(
        train.documents, test.documents, oov, pad)
    ytrain, ytest, label2index, index2label = preprocess_labels(
        train.labels, test.labels)

    if en_emb is not None:
        index2emb = get_embeddings(tokenizer, embed_en, embed_es)
        embedding_layer = get_embedding_layer(index2emb, max_len, tokenizer)

    ################# Model Making and Training #################

    vocab_size = len(tokenizer.word_index) + 1
    print('vocab size: {}'.format(vocab_size))

    if en_emb is not None:
        model = make_LSTM(embedding_layer, max_len)
        print(model.summary())

        history = model.fit(Xtrain, ytrain, batch_size=256,
                            epochs=20, verbose=1, validation_split=0.1)

        predictions = model.predict(Xtest)
        pred = np.argmax(predictions, axis=1)
        Ytest_converted = np.argmax(ytest, axis=1)

        print(classification_report(Ytest_converted,
                                    pred, target_names=['neg', 'neu', 'pos']))
        cm = confusion_matrix(Ytest_converted, pred)
        print_cm(cm, ["negative", "neutral", "positive"])

    else:
        model_wE = make_LSTM_with_embeddings(vocab_size, max_len)
        model_wE.summary()

        history_2 = model_wE.fit(
            Xtrain, ytrain, batch_size=256, epochs=20, verbose=1, validation_split=0.1)

        predictions = model_wE.predict(Xtest)
        pred = np.argmax(predictions, axis=1)
        Ytest_converted = np.argmax(ytest, axis=1)

        print(classification_report(Ytest_converted,
                                    pred, target_names=['neg', 'neu', 'pos']))
        cm = confusion_matrix(Ytest_converted, pred)
        print_cm(cm, ["negative", "neutral", "positive"])


if __name__ == '__main__':
    data_root("./data/")
